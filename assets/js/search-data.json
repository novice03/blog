{
  
    
        "post0": {
            "title": "Hello, World",
            "content": "Hello, World program in Whitespace . Hello, World program . Reference: https://github.com/rdebath/whitespace/blob/master/tests/rdebath/helloworld.ws .",
            "url": "https://novice03.github.io/blog/2021/05/31/hello-world.html",
            "relUrl": "/2021/05/31/hello-world.html",
            "date": " • May 31, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "Multi-arm bandit Problem . A problem in which an action is taken only in one situation (non-associative). | An agent must choose from k options. | Each chosen action (option) gives a reward from a probability distribution specific to that option. | The goal is to maximize the expected total reward over multiple action selections (time steps). | . import numpy import matplotlib.pyplot as plt . k = 10 action_values = numpy.random.randn(k) # The average reward for each of the options. Used as mean in p.d. while choosing reward. print(action_values) steps = 500 rewards = [] . [-0.9622023 1.84491299 -1.61877345 0.48772829 1.33372346 -0.46839888 -0.00491636 -1.59817654 0.93799994 -2.27956891] . def reward(action): # generates random reward for a specific action mu, var = action_values[action - 1] ,1 return numpy.random.normal(mu, var, 1) . The value of an action (expected reward when the action is selected) is denoted by $ q_*(a) = mathbb{E}[ R_t | A_t = a ]$ . The action values are stored in the action_values array. It is assumed that these values are not known, but can be estimated with some certainty. We use these estimates to choose actions. Such methods for estimating action values and using these values to influence actions are called action-value methods. . One way to estimate the action value is to calculate the mean of rewards earned for that action until the current time step. The approximation $ Q_t(a) = frac{ text{sum of rewards earned when action } a text{ taken prior to }t}{ text{number of times a taken prior to }t}$ would converge to the action values as the number of time steps approches infinity. . predicted_action_values = numpy.zeros((2, k)) print(predicted_action_values) . [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] . def update_predicted_action_values(action, reward): action_index = action - 1 current_prediction = predicted_action_values[0][action_index] times_chosen = predicted_action_values[1][action_index] #updated_prediction = current_prediction + (reward - current_prediction) / (times_chosen + 1) updated_prediction = (current_prediction * times_chosen + reward) / (times_chosen + 1) updated_times_chosen = times_chosen + 1 predicted_action_values[0][action_index] = updated_prediction predicted_action_values[1][action_index] = updated_times_chosen . Epsilon greedy actions . One approach to the problem is to always choose an action that maximizes $Q_t(a)$. Such greedy actions do not explore seemingly low reward actions for high rewards. Therefore, with a random probability of $ epsilon$, an action can be chosen randomly. . def choose_best_action(action_values): return numpy.argmax(action_values[0]) + 1 def choose_random_action(action_values): return int(numpy.where(action_values[0] == numpy.random.choice(action_values[0], 1))[0][0]) + 1 . def choose_action(action_values, eps): t = [0, 1] choice = numpy.random.choice(t, size = 1, p = [eps, 1 - eps]) if choice == 0: return choose_random_action(action_values) else: return choose_best_action(action_values) . def time_step(action_values, eps): chosen_action = choose_action(action_values, eps) current_reward = reward(chosen_action) update_predicted_action_values(chosen_action, current_reward) return current_reward . rewards = [] mean_rewards = [] approximation_error = [] for step in range(steps): reward_gained = time_step(predicted_action_values, eps = 0.1) rewards.append(reward_gained) mean_rewards.append(numpy.mean(rewards)) approximation_error.append(numpy.absolute(numpy.sum(numpy.absolute(predicted_action_values)[0] - numpy.absolute(action_values)))) . plt.plot(range(steps), mean_rewards) plt.plot(range(steps), approximation_error) . [&lt;matplotlib.lines.Line2D at 0x2674268b820&gt;] . print(numpy.max(action_values)) . 1.8449129935143849 . The average reward gained over all the time steps converges to the highest action value. As the number of steps increases, the action with the highest value is chosen everytime unless a random action is chosen. .",
            "url": "https://novice03.github.io/blog/2021/05/27/multi-armed-bandits.html",
            "relUrl": "/2021/05/27/multi-armed-bandits.html",
            "date": " • May 27, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "novice03 is a machine learning novice. .",
          "url": "https://novice03.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://novice03.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}